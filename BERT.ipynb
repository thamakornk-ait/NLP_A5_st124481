{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bSz5jzj61nHc"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import important library\n",
        "import math\n",
        "import re\n",
        "from random import *\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data\n",
        "\n",
        "https://edition.cnn.com/2024/02/24/business/dogs-need-coats-curious-consumer-wellness/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "#read the file\n",
        "import spacy\n",
        "\n",
        "with open(\"pets_news.txt\", \"r\", encoding='utf-8') as f:\n",
        "    raw_text = f.read()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing\n",
        "\n",
        "### Tokenization and numericalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#tokenization\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(raw_text)\n",
        "sentences = list(doc.sents)\n",
        "\n",
        "#lower case, and clean all the symbols\n",
        "text = [x.text.lower() for x in sentences]\n",
        "text = [re.sub(\"[.,!?\\\\-]\", '', x) for x in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "#making vocabs - numericalization\n",
        "word_list = list(set(\" \".join(text).split()))\n",
        "word2id   = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, w in enumerate(word_list):\n",
        "    word2id[w] = i + 4 #reserve the first 0-3 for CLS, PAD\n",
        "    id2word    = {i:w for i, w  in enumerate(word2id)}\n",
        "    vocab_size = len(word2id)\n",
        "    \n",
        "token_list = list()\n",
        "for sentence in text:\n",
        "    arr = [word2id[word] for sentence in text for word in sentence.split()]\n",
        "    token_list.append(arr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 6 #batch size =6\n",
        "max_mask   = 5 \n",
        "max_len    = 3909 #maximum length that my transformer will accept.....all sentence will be padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_batch():\n",
        "    batch = []\n",
        "    positive = negative = 0\n",
        "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
        "        \n",
        "        #randomly choose two sentence\n",
        "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
        "        tokens_a, tokens_b            = token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "        \n",
        "        #1. token embedding - add CLS and SEP\n",
        "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
        "        \n",
        "        #2. segment embedding - which sentence is 0 and 1\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "        \n",
        "        #3 masking\n",
        "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
        "        #get all the pos excluding CLS and SEP\n",
        "        candidates_masked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] \n",
        "                                 and token != word2id['[SEP]']]\n",
        "        shuffle(candidates_masked_pos)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        #simply loop and mask accordingly\n",
        "        for pos in candidates_masked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            if random() < 0.1:  #10% replace with random token\n",
        "                index = randint(0, vocab_size - 1)\n",
        "                input_ids[pos] = word2id[id2word[index]]\n",
        "            elif random() < 0.8:  #80 replace with [MASK]\n",
        "                input_ids[pos] = word2id['[MASK]']\n",
        "            else: \n",
        "                pass\n",
        "            \n",
        "        #4. pad the sentence to the max length\n",
        "        n_pad = max_len - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "        \n",
        "        #5. pad the mask tokens to the max length\n",
        "        if max_mask > n_pred:\n",
        "            n_pad = max_mask - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "        \n",
        "        #6. check whether is positive or negative\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
        "            negative += 1\n",
        "        \n",
        "    return batch\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = make_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([6, 3909]),\n",
              " torch.Size([6, 3909]),\n",
              " torch.Size([6, 5]),\n",
              " torch.Size([6, 5]),\n",
              " tensor([0, 0, 0, 1, 1, 1]))"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# masked_tokens"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Embedding\n",
        "\n",
        "<img src = \"figures/BERT_embed.png\" width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        #x, seg: (bs, len)\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "s1PGksqBNuZM"
      },
      "outputs": [],
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([6, 3909, 3909])\n"
          ]
        }
      ],
      "source": [
        "print(get_attn_pad_mask(input_ids, input_ids).shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Encoder\n",
        "\n",
        "The encoder has two main components: \n",
        "\n",
        "- Multi-head Attention\n",
        "- Position-wise feed-forward network\n",
        "\n",
        "First let's make the wrapper called `EncoderLayer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn       = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "        return enc_outputs, attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the scaled dot attention, to be used inside the multihead attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the parameters first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_layers = 6    # number of Encoder of Encoder Layer\n",
        "n_heads  = 8    # number of heads in Multi-Head Attention\n",
        "d_model  = 768  # Embedding Size\n",
        "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_segments = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the Multiheadattention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "\n",
        "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the PoswiseFeedForwardNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
        "        return self.fc2(F.gelu(self.fc1(x)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4 Putting them together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "OZ0TJ84W4SZw"
      },
      "outputs": [],
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.activ = nn.Tanh()\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        # decoder is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
        "        \n",
        "        # 1. predict next sentence\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
        "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
        "\n",
        "        # 2. predict the masked token\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
        "\n",
        "        return logits_lm, logits_nsp"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UAG3SEP4UbU",
        "outputId": "bc6f202f-df37-4fac-843c-fb86bdb777b2"
      },
      "outputs": [],
      "source": [
        "# best_loss = float('inf')\n",
        "# num_epoch = 100\n",
        "model = BERT()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# save_path = f'models/{model.__class__.__name__}2.pt'\n",
        "\n",
        "# batch = make_batch()\n",
        "# input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "\n",
        "# for epoch in range(num_epoch):\n",
        "#     optimizer.zero_grad()\n",
        "#     logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
        "#     #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
        "#     #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
        "\n",
        "#     #1. mlm loss\n",
        "#     #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
        "#     loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "#     loss_lm = (loss_lm.float()).mean()\n",
        "#     #2. nsp loss\n",
        "#     #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
        "#     loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
        "    \n",
        "#     #3. combine loss\n",
        "#     loss = loss_lm + loss_nsp\n",
        "#     if epoch % 1 == 0:\n",
        "#         print('Epoch:', '%02d' % (epoch), 'loss =', '{:.6f}'.format(loss))\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "#     if loss < best_loss:\n",
        "#         best_loss = loss\n",
        "#         torch.save(model.state_dict(), save_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference\n",
        "\n",
        "Since our dataset is very small, it won't work very well, but just for the sake of demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BERT(\n",
              "  (embedding): Embedding(\n",
              "    (tok_embed): Embedding(852, 768)\n",
              "    (pos_embed): Embedding(3909, 768)\n",
              "    (seg_embed): Embedding(2, 768)\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (layers): ModuleList(\n",
              "    (0-5): 6 x EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (W_Q): Linear(in_features=768, out_features=512, bias=True)\n",
              "        (W_K): Linear(in_features=768, out_features=512, bias=True)\n",
              "        (W_V): Linear(in_features=768, out_features=512, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PoswiseFeedForwardNet(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (activ): Tanh()\n",
              "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (decoder): Linear(in_features=768, out_features=852, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the saved weights\n",
        "saved_weights_path = './models/BERT2.pt'  # Path to your saved model weights\n",
        "model.load_state_dict(torch.load(saved_weights_path))\n",
        "model.eval()  # Put model in evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3K8T6B4YJp",
        "outputId": "e057405b-1f78-431c-fa71-032a738fb848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'the', 'petproducts', 'market', 'is', 'booming', 'but', 'does', 'your', 'dog', 'need', 'a', 'coat', 'in', 'the', 'thick', 'of', 'a', 'minnesota', 'winter', 'it’s', 'not', 'uncommon', 'to', 'see', 'dogs', 'especially', 'those', 'of', 'the', 'shorthaired', 'sinewy', 'or', 'small', 'variety', 'sporting', 'outerwear', 'on', 'their', 'neighborhood', 'strolls', 'or', 'jaunts', 'around', 'the', 'state’s', 'many', 'lakes', 'while', 'the', 'idea', 'of', 'canine', 'couture', 'may', 'conjure', 'images', 'of', 'halloween', 'costumes', 'or', 'society', 'dogs', 'dressed', 'to', 'the', 'nines', 'dog', 'coats', 'have', 'become', 'a', 'serious', 'business:', 'not', 'only', 'are', 'they', 'a', 'sturdy', 'leg', 'of', 'a', 'booming', 'multibilliondollar', 'pet', 'products', 'industry', 'but', 'they', 'also', 'symbolize', 'the', 'evolution', 'of', 'humans’', 'relationships', 'with', 'their', 'pets', 'but', 'all', 'this', 'begs', 'the', 'question:', 'does', 'your', 'dog', 'need', 'a', 'coat', 'what', 'the', 'doc', 'says', 'the', 'short', 'answer:', 'it', 'depends', 'on', 'a', 'variety', 'of', 'factors', 'including', 'a', 'dog’s', 'breed', 'its', 'own', 'coat', 'size', 'age', 'health', 'and', 'circumstance', 'dr', 'jerry', 'klein', 'chief', 'veterinary', 'officer', 'at', 'the', 'american', 'kennel', 'club', 'told', 'cnn', 'smaller', 'dogs', 'are', 'typically', 'more', 'sensitive', 'to', 'cold', 'than', 'larger', 'dogs', 'especially', 'being', 'closer', 'to', 'the', 'ground', 'klein', 'said', 'a', 'dog', 'named', 'godiva', 'was', 'bundled', 'against', 'the', 'cold', 'with', 'her', 'human', 'as', 'they', 'walked', 'through', 'boston', 'common', 'in', 'boston', 'massachusetts', 'on', 'december', '25', '2022', 'a', 'dog', 'named', 'godiva', 'was', 'bundled', 'against', 'the', 'cold', 'with', 'her', 'human', 'as', 'they', 'walked', 'through', 'boston', 'common', 'in', 'boston', 'massachusetts', 'on', 'december', '25', '2022', 'jessica', 'rinaldi/the', 'boston', 'globe/getty', 'images', 'the', 'breeds’', 'type', 'of', 'fur', 'whether', 'singlecoated', 'doublecoated', 'hairless', 'or', 'thinning', 'is', 'a', 'significant', 'factor', 'as', 'doublecoated', 'dogs', 'have', 'an', 'undercoat', 'that', 'helps', 'keep', 'them', 'warm', 'in', 'the', 'winter', 'geriatric', 'dogs', 'and', 'very', 'young', 'ones', 'benefit', 'from', 'the', 'additional', 'warmth', 'especially', 'if', 'they', 'have', 'underlying', 'health', 'issues', 'that', 'affect', 'how', 'they', 'can', 'regulate', 'temperature', 'or', 'if', 'they’re', 'diabetic', 'or', 'have', 'osteoarthritis', 'he', 'said', '“trust', 'me', 'i’m', 'a', '70yearold', 'man;', 'i', 'don’t', 'like', 'the', 'cold', 'so', 'much', 'anymore', 'either”', 'he', 'said', 'from', 'highfashion', 'to', 'highperformance', 'there’s', 'evidence', 'of', 'dog', 'fashion', 'and', 'clothing', 'throughout', 'history', 'from', 'the', 'ancient', 'egyptians', 'and', '19thcentury', 'paris', 'dog', 'boutiques', 'to', 'early', '20thcentury', 'photographs', 'of', 'dogs', 'wearing', 'clothes', 'but', 'what', 'they', 'were', 'wearing', 'changed', 'radically', 'over', 'time', 'a', 'fitting', 'parallel', 'can', 'be', 'found', 'in', 'the', 'history', 'and', 'evolution', 'of', 'the', 'dog', 'collar', 'said', 'alan', 'fausel', 'curator', 'of', 'the', 'akc', 'museum', 'of', 'the', 'dog', 'in', 'new', 'york', 'city', 'which', 'last', 'year', 'hosted', 'the', 'exhibit', '“identity', '&', 'restraint:', 'art', 'of', 'the', 'dog', 'collar”', 'this', 'illustration', 'dated', 'from', 'circa', '1880', '1895', 'shows', 'a', 'woman', 'standing', 'in', 'front', 'of', 'the', 'window', 'of', 'enoch', 'frères', '&', 'costallat', 'publishers', 'in', 'paris', 'with', 'in', 'the', 'foreground', 'a', 'poodle', 'this', 'illustration', 'dated', 'from', 'circa', '1880', '1895', 'shows', 'a', 'woman', 'standing', 'in', 'front', 'of', 'the', 'window', 'of', 'enoch', 'frères', '&', 'costallat', 'publishers', 'in', 'paris', 'with', 'in', 'the', 'foreground', 'a', 'poodle', 'sepia', 'times/universal', 'images', 'group/getty', 'images', 'the', 'traveling', 'exhibition', 'of', '187', 'collars', 'showed', 'the', 'progression', 'of', 'dogs', 'from', 'utilitarian', 'hunters', 'and', 'helpers', 'in', 'the', 'field', 'to', 'indoor', 'members', 'of', 'the', 'family', '“you', 'see', 'these', 'paintings', 'from', 'the', '17th', 'century', 'often', 'difficult', 'in', 'the', 'museum', 'to', 'display', 'that', 'are', 'of', 'bloody', 'violent', 'dogs”', 'fausel', 'said', '“then', 'you', 'turn', 'a', 'couple', 'of', 'pages', '[in', 'an', 'art', 'book]', 'and', 'all', 'of', 'a', 'sudden', 'there', 'are', 'these', 'same', 'dogs', 'in', 'your', 'living', 'room', 'so', 'it’s', 'domesticity', 'that', 'changes', 'the', 'arc', 'of', 'the', 'manufacture', 'and', 'the', 'quality', 'of', 'the', 'item”', 'in', 'the', '1600s', 'when', 'dogs', 'were', 'used', 'in', 'pack', 'hunts', 'of', 'large', 'quarry', '(bears', 'boars', 'stags', 'etc)', 'the', 'canines', 'would', 'wear', 'spiked', 'iron', 'collars', 'for', 'protection', 'in', 'the', 'coming', 'centuries', 'collars', 'took', 'on', 'a', 'softer', 'decorative', 'form', 'and', 'were', 'made', 'from', 'materials', 'such', 'as', 'brass', 'tiffany', 'silver', 'and', 'gemstones', 'in', 'recent', 'decades', 'there', 'have', 'been', 'some', 'memorable', 'canine', 'attire', 'examples', 'in', 'popular', 'culture:', 'bruiser', 'woods’', 'fetching', 'pinkhued', 'wardrobe', 'in', '“legally', 'blonde;”', 'frank', 'the', 'pug’s', 'sharp', 'suit', '&', 'tie', 'in', 'the', '“men', 'in', 'black”', 'films', 'or', 'eddie’s', 'bright', 'yellow', 'rain', 'slicker', 'on', 'a', 'stormy', 'seattle', 'episode', 'of', '“frasier”', 'and', 'scrolls', 'through', 'tiktok', 'and', 'instagram', 'today', 'show', 'fashionistas', 'and', 'paw', 'parents', 'alike', 'playing', 'dressup', 'with', 'their', 'pet', 'squirrels', 'miniature', 'pigs', 'and', 'the', 'most', 'tolerant', 'of', 'cats', 'the', 'dachshund', 'wears', 'prada', 'the', 'dog', 'apparel', 'business', 'of', '2024', 'offers', 'many', 'choices', 'and', 'caters', 'to', 'all', 'kinds', '[MASK]', 'pet', 'owners:', 'one', 'could', 'pony', 'up', 'for', 'a', '$1000', 'gucci', 'or', 'prada', 'coat', 'or', 'spend', 'a', 'couple', 'of', 'ten', 'spots', 'for', 'a', 'weatherproof', 'coat', 'on', 'chewy', \"prada's\", 'renylon', 'puffer', 'dog', 'coat', 'with', 'hood', 'comes', 'in', 'two', 'colors', 'and', 'three', 'sizes', '$825', \"prada's\", 'renylon', 'puffer', 'dog', 'coat', 'with', 'hood', 'comes', 'in', 'two', 'colors', 'and', 'three', 'sizes', '$825', 'from', 'prada', 'but', 'the', 'transition', 'from', 'fifth', 'avenue', 'highend', 'and', 'high', 'fashion', 'to', 'highperformance', 'and', 'technical', 'function', 'has', 'certainly', 'accelerated', 'in', 'recent', 'decades', 'bend', 'oregonbased', 'ruffwear', 'is', 'one', 'member', 'of', 'a', 'large', 'pack', 'of', 'firms', 'that', 'sit', 'at', 'the', 'intersection', 'of', 'the', 'pet', 'products', 'industry', 'and', 'the', 'outdoor', 'gear', 'industry', '“we', 'approach', 'the', 'dog', 'apparel', 'as', 'we', 'would', 'outdoor', 'apparel', 'for', 'humans', 'in', 'the', 'sense', 'that', 'we’re', 'looking', 'for', 'outdoor', 'fabrications', 'outdoor', 'innovations', 'and', 'technologies', 'and', 'ways', 'to', 'be', 'able', 'to', 'contribute', 'to', 'the', 'comfort', 'of', 'any', 'of', 'our', 'dogs”', 'said', 'peter', 'kearns', 'ruffwear’s', 'director', 'of', 'product', 'batman', 'the', 'dog', 'and', 'his', 'human', 'in', 'the', 'backbone', 'trail', 'of', 'the', 'santa', 'monica', 'mountains', 'batman', 'wears', 'a', 'ruffwear', 'swamp', 'cooler', 'zip', 'vest', 'and', 'apparel', 'batman', 'the', 'dog', 'and', 'his', 'human', 'in', 'the', 'backbone', 'trail', 'of', 'the', 'santa', 'monica', 'mountains', 'batman', 'wears', 'a', 'ruffwear', 'swamp', 'cooler', 'zip', 'vest', 'and', 'apparel', 'courtesy', 'ruffwear', '“there’s', 'this', 'continued', 'trend', 'around', 'dogs', 'as', 'family”', 'he', 'added', '“and', 'when', 'taking', 'them', 'on', 'vacation', '[or', 'other', 'excursions]', 'you', 'need', 'to', 'be', 'prepared', 'and', 'you', 'need', 'to', 'prepare', 'your', 'dog”', 'pets', 'and', 'the', 'pandemic', 'since', '2020', 'that', 'trend', 'has', 'intensified', 'the', 'covid19', 'pandemic', 'resulted', 'in', 'people', 'spending', 'more', 'time', 'at', 'home', 'with', 'their', 'pets', 'as', 'well', 'as', 'adopting', 'new', 'companions;', 'as', 'a', 'result', 'pet', 'product', 'sales', 'boomed', 'prepandemic', 'spending', 'on', 'pets', 'had', 'increased', 'slowly', 'and', 'steadily', 'following', 'the', 'great', 'recession', 'growing', 'from', '$46', 'billion', 'in', '2009', 'to', '$75', 'billion', 'in', '2019', 'according', 'to', 'the', 'american', 'pet', 'products', 'association', 'the', 'pandemic', 'effect', 'was', 'huge:', 'spending', 'soared', 'to', '$104', 'billion', 'in', '2020', 'it’s', 'since', 'bounded', 'up', 'to', '$148', 'billion', 'to', 'end', '2023', 'and', 'is', 'expected', 'to', 'hit', 'nearly', '$280', 'billion', 'in', '2030', 'according', 'to', 'the', 'appa', 'which', 'does', 'not', 'break', 'out', 'apparel', 'sales', 'voyagers', 'k9', 'apparel', 'a', 'tacoma', 'washingtonbased', 'maker', 'of', 'jackets', 'for', '25', 'dog', 'breeds', 'has', 'seen', 'some', 'tailwinds', 'from', 'the', 'pandemic', 'three', 'dogs', 'in', 'voyagers', 'k9', 'apparel', 'yellow', 'raincoats', 'on', 'a', 'rocky', 'beach', 'on', 'a', 'grey', 'day', 'three', 'dogs', 'in', 'voyagers', 'k9', 'apparel', 'yellow', 'raincoats', 'on', 'a', 'rocky', 'beach', 'on', 'a', 'grey', 'day', 'courtesy', 'robyn', 'baldrey/@reddogsquad', '“people', 'were', 'adopting', 'more', 'dogs', 'and', 'people', 'were', 'home', 'with', 'their', 'dogs', 'more', 'but', 'the', 'other', 'one', 'that', 'i', 'think', 'we', 'sometimes', 'overlook', 'and', 'is', 'really', 'more', 'core', 'to', 'our', 'business', 'is', 'how', 'much', 'people', 'were', 'getting', 'outside', 'with', 'their', 'dogs”', 'said', 'joe', 'hafner', 'who', 'bought', 'the', 'company', 'last', 'year', 'from', 'its', 'longtime', 'owners', '“postpandemic', 'people', 'continue', 'to', 'hold', 'onto', 'that', 'love', 'and', 'respect', 'for', 'nature', 'and', 'taking', 'their', 'dogs', 'with', 'them', 'on', 'those', 'long', 'walks', 'those', 'long', 'hikes”', 'he', 'added', 'in', 'addition', 'to', 'the', 'simmering', 'consumer', 'trends', 'the', 'shifting', 'climate', 'patterns', '(including', 'truncated', 'winters)', 'and', 'more', 'extreme', 'weather', 'events', 'have', 'prompted', 'voyagers', 'to', 'introduce', 'a', 'line', 'of', 'cooling', 'coats', 'this', 'summer', 'he', 'said', '“if', 'your', 'intent', 'with', 'your', 'dog', 'is', 'to', 'protect', 'them', 'from', 'weather', 'and', 'protect', 'them', 'from', 'extreme', 'heat', 'or', 'extreme', 'cold', 'there’s', 'a', 'place', 'in', 'the', 'market', 'for', 'functional', 'wear', 'of', 'that', 'type”', 'hafner', 'said', '“that', 'being', 'said', 'there’s', 'plenty', 'of', 'great', 'reasons', 'to', 'dress', 'your', 'dog', 'up', 'for', 'halloween', 'or', 'a', 'walk', 'around', 'new', 'york', 'city', 'in', 'their', 'stately', 'duds', 'that’s', 'not', 'the', 'market', 'that', 'we', 'are', 'serving', 'but', 'i', 'think', 'there’s', 'certainly', 'a', 'place', 'for', 'that', 'too”', 'coats', 'for', 'peace', 'of', 'mind', 'at', 'nature', 'of', 'the', 'dog', 'boarding', 'and', 'daycare', 'facility', 'in', 'ada', 'michigan', 'owner', 'jackie', 'moord', 'has', 'seen', 'a', 'steady', 'increase', 'of', 'the', 'coatwearing', 'canine', 'clientele', '“there', 'are', 'a', 'lot', 'of', 'different', 'styles', 'for', 'these', 'jackets', 'now', 'so', 'maybe', 'it’s', 'some', 'of', 'that', 'which', 'is', 'kind', 'of', 'fun', '(for', 'owners)”', 'she', 'said', '“but', 'mainly', 'i', 'think', 'it', 'creates', 'peace', 'of', 'mind', 'for', 'people', 'sending', 'your', 'dog', 'to', 'daycare', 'in', 'a', 'coat', 'just', 'kind', 'of', 'helps', 'with', 'that', 'peace', 'of', 'mind', 'that', 'they’re', 'taking', 'good', 'care', 'of', 'the', 'dog”', 'three', 'chihuahuas', 'wearing', 'matching', 'coats', 'watch', 'their', 'owner', 'from', 'a', 'stroller', 'on', 'november', '27', '2020', 'in', 'rehoboth', 'beach', 'delaware', 'three', 'chihuahuas', 'wearing', 'matching', 'coats', 'watch', 'their', 'owner', 'from', 'a', 'stroller', 'on', 'november', '27', '2020', 'in', 'rehoboth', 'beach', 'delaware', 'mark', 'makela/getty', 'images', 'in', 'anoka', 'minnesota', 'nicole', 'wheatley', 'has', 'made', 'custom', 'fleece', 'pullovers', 'for', 'dogs', 'since', '2012', 'that’s', 'when', 'she', 'crafted', 'her', 'first', 'coat', 'purely', 'out', 'of', 'necessity', 'when', 'her', 'doberman', 'shanoa', 'managed', 'to', 'chew', 'up', 'an', 'expensive', 'winter', 'coat', '“and', 'i', 'thought', '‘i', 'can’t', 'spend', 'that', 'kind', 'of', 'money', 'again’”', 'wheatley', 'said', 'the', 'selftaught', 'seamstress', 'sewed', 'together', 'some', 'fleece', 'to', 'keep', 'shanoa', 'warm', 'and', 'shared', 'the', 'final', 'results', 'on', 'the', 'doberman', 'owners’', 'forum', 'she', 'comoderated', '“they', 'were', 'like', '‘oh', 'those', 'look', 'nice', 'could', 'you', 'make', 'one', 'for', 'me’”', 'she', 'said', '“and', 'then', 'it', 'just', 'kind', 'of', 'spiraled”', 'homespun', 'threads', 'flashforward', 'to', '2024', 'and', 'the', 'onewoman', 'shop', 'of', 'made', 'by', 'meadowcat', 'is', 'now', 'wheatley’s', 'fulltime', 'job', 'she', 'produces', 'about', '1000', 'to', '1300', 'custom', 'coats', 'annually', 'priced', 'generally', 'between', '$40', 'and', '$75', 'that', 'are', 'made', 'for', 'a', 'variety', 'of', 'dogs', 'including', 'some', 'unlikely', 'breeds', 'for', 'example', 'wheatley', 'crafted', 'a', 'bright', 'fluorescent', 'orange', 'coat', 'for', 'a', 'resident', 'in', 'minnesota’s', 'northwoods', 'who', 'feared', 'their', 'husky', 'could', 'get', 'mistaken', 'for', 'a', 'wolf', 'over', 'in', 'muncy', 'pennsylvania', 'a', 'doberman', 'named', 'drogo', 'has', 'more', 'than', '50', 'made', 'by', 'meadowcat', 'coats', 'said', 'lois', 'katchur', 'who', 'was', 'one', 'of', 'wheatley’s', 'first', 'customers', 'two', 'dobermans', 'in', 'made', 'by', \"meadowcat's\", 'blue', 'and', 'red', 'coordinating', 'coats', 'sypha', 'on', 'the', 'left', 'and', 'richter', 'on', 'the', 'right', 'two', 'dobermans', 'in', 'made', 'by', \"meadowcat's\", 'blue', 'and', 'red', 'coordinating', 'coats', 'sypha', 'on', 'the', 'left', 'and', 'richter', 'on', 'the', 'right', 'courtesy', 'nicole', 'wheatley', 'some', 'of', 'drogo’s', 'coats', 'are', 'handmedowns', 'from', 'katchur’s', 'previous', 'dobermans', 'and', 'many', 'are', 'tailored', 'for', 'the', '88pound', 'pup', 'named', 'after', 'the', '“game', 'of', 'thrones”', 'dothraki', 'warlord', '(one', 'coat', 'fittingly', 'features', 'firebreathing', 'dragons)', 'doberman', 'pinschers', 'along', 'with', 'sleek', 'breeds', 'like', 'greyhounds', 'and', 'whippets', 'are', 'prime', 'candidates', 'for', 'dog', 'coats', 'because', 'of', 'their', 'short', 'hair', 'thin', 'skin', 'and', 'minimal', 'body', 'fat', 'on', 'a', 'recent', 'february', 'morning', 'in', 'wheatley’s', 'home', 'office', 'her', 'dobermans', 'leon', 'and', 'sypha', 'contently', 'and', 'gently', 'chewed', 'on', 'handmade', 'fleece', 'patchwork', 'blanket', 'toys', 'sypha', 'did', 'so', 'wearing', 'a', 'grumpy', 'cat', 'pullover', '“she', 'was', 'chilly', 'this', 'morning”', 'wheatley', 'explained', '“she', 'was', 'sitting', 'next', 'to', 'me', 'shivering', 'and', 'looking', 'at', 'me”', 'tips', 'for', 'pet', 'parents', 'dr', 'jennifer', 'bruns', 'doctor', 'of', 'veterinary', 'medicine', 'at', 'petsmart', 'veterinary', 'services', 'said', 'it’s', 'important', 'to', 'monitor', 'your', 'dog', 'for', 'signs', 'such', 'as', 'shivering', 'or', 'general', 'discomfort', 'to', 'determine', 'when', 'a', 'coat', 'is', 'in', 'need', 'a', 'woman', 'runs', 'with', 'her', 'dog', 'along', 'the', 'snowcovered', 'national', 'mall', 'in', 'january', '2022', 'in', 'washington', 'dc', 'a', 'woman', 'runs', 'with', 'her', 'dog', 'along', 'the', 'snowcovered', 'national', 'mall', 'in', 'january', '2022', 'in', 'washington', 'dc', 'kent', 'nishimura/los', 'angeles', 'times/getty', 'images', '“in', 'all', 'situations', 'introduce', 'the', 'coat', 'slowly', 'as', 'it', 'might', 'take', 'a', 'few', 'times', 'for', 'your', 'dog', 'to', 'get', 'adjusted', 'to', 'wearing', 'it”', 'bruns', 'wrote', 'via', 'email', '“make', 'sure', 'the', 'coat', 'fits', 'well', 'for', 'safety', 'and', 'they’re', 'supervised', 'while', 'wearing', 'the', 'coat', 'consider', 'coats', 'with', 'reflectors', 'for', 'nighttime', 'safety”', 'coats', 'also', 'can', 'help', 'calm', 'a', 'dog', 'during', 'anxietyinducing', 'situations', 'like', 'the', 'fourth', 'of', 'july', 'or', 'keep', 'dry', 'during', 'rainstorms', 'beyond', 'coats', 'winter', 'wear', 'such', 'as', 'boots', 'are', 'beneficial', 'in', 'protecting', 'paws', 'from', 'frostbite', 'or', 'injuries', 'from', 'chemical', 'deicers', 'klein', 'added', 'akc', 'has', 'additional', 'tips', 'on', 'its', 'website', 'but', 'also', 'recommends', 'to', 'never', 'leave', 'your', 'dog', 'unattended', 'in', 'a', 'car', 'no', 'matter', 'the', 'season', '[SEP]', 'the', 'petproducts', 'market', 'is', 'booming', 'but', 'does', 'your', 'dog', 'need', 'a', 'coat', 'in', 'the', 'thick', 'of', 'a', 'minnesota', 'winter', 'it’s', 'not', 'uncommon', 'to', 'see', 'dogs', 'especially', 'those', 'of', 'the', 'shorthaired', 'sinewy', 'or', 'small', 'variety', 'sporting', 'outerwear', 'on', 'their', 'neighborhood', 'strolls', 'or', 'jaunts', 'around', 'the', 'state’s', 'many', 'lakes', 'while', 'the', 'idea', 'of', 'canine', 'couture', 'may', 'conjure', 'images', 'of', 'halloween', 'costumes', 'or', 'society', 'dogs', 'dressed', 'to', 'the', 'nines', 'dog', 'coats', 'have', 'become', 'a', 'serious', 'business:', 'not', 'only', 'are', 'they', 'a', 'sturdy', 'leg', 'of', 'a', 'booming', 'multibilliondollar', 'pet', 'products', 'industry', 'but', 'they', 'also', 'symbolize', 'the', 'evolution', 'of', 'humans’', 'relationships', 'with', 'their', 'pets', 'but', 'all', 'this', 'begs', 'the', 'question:', 'does', 'your', 'dog', 'need', 'a', 'coat', 'what', 'the', 'doc', 'says', 'the', 'short', 'answer:', 'it', 'depends', 'on', 'a', 'variety', 'of', 'factors', 'including', 'a', 'dog’s', 'breed', 'its', 'own', 'coat', 'size', 'age', 'health', 'and', 'circumstance', 'dr', 'jerry', 'klein', 'chief', 'veterinary', 'officer', 'at', 'the', 'american', 'kennel', 'club', 'told', 'cnn', 'smaller', 'dogs', 'are', 'typically', 'more', 'sensitive', 'to', 'cold', 'than', 'larger', 'dogs', 'especially', 'being', 'closer', 'to', 'the', 'ground', 'klein', 'said', 'a', 'dog', 'named', 'godiva', 'was', 'bundled', 'against', 'the', 'cold', 'with', 'her', 'human', 'as', 'they', 'walked', 'through', 'boston', 'common', 'in', 'boston', 'massachusetts', 'on', 'december', '25', '2022', 'a', 'dog', 'named', 'godiva', 'was', 'bundled', 'against', 'the', 'cold', 'with', 'her', 'human', 'as', 'they', 'walked', 'through', 'boston', 'common', 'in', 'boston', 'massachusetts', 'on', 'december', '25', '2022', 'jessica', 'rinaldi/the', 'boston', 'globe/getty', 'images', 'the', 'breeds’', 'type', 'of', 'fur', 'whether', 'singlecoated', 'doublecoated', 'hairless', 'or', 'thinning', 'is', 'a', 'significant', 'factor', 'as', 'doublecoated', 'dogs', 'have', 'an', 'undercoat', 'that', 'helps', 'keep', 'them', 'warm', 'in', 'the', 'winter', 'geriatric', 'dogs', 'and', 'very', 'young', 'ones', 'benefit', 'from', 'the', 'additional', 'warmth', 'especially', 'if', 'they', 'have', 'underlying', 'health', 'issues', 'that', 'affect', 'how', 'they', 'can', 'regulate', 'temperature', 'or', 'if', 'they’re', 'diabetic', 'or', 'have', 'osteoarthritis', 'he', 'said', '“trust', 'me', 'i’m', 'a', '70yearold', 'man;', 'i', 'don’t', 'like', 'the', 'cold', 'so', 'much', 'anymore', 'either”', 'he', 'said', 'from', 'highfashion', 'to', 'highperformance', 'there’s', 'evidence', 'of', 'dog', 'fashion', 'and', 'clothing', 'throughout', 'history', 'from', 'the', 'ancient', 'egyptians', 'and', '19thcentury', 'paris', 'dog', 'boutiques', 'to', 'early', '20thcentury', 'photographs', 'of', 'dogs', 'wearing', 'clothes', 'but', 'what', 'they', 'were', 'wearing', 'changed', 'radically', 'over', 'time', 'a', 'fitting', 'parallel', 'can', 'be', 'found', 'in', 'the', 'history', 'and', 'evolution', 'of', 'the', 'dog', 'collar', 'said', 'alan', 'fausel', 'curator', 'of', 'the', 'akc', 'museum', 'of', 'the', 'dog', 'in', 'new', 'york', 'city', 'which', 'last', 'year', 'hosted', 'the', 'exhibit', '“identity', '&', 'restraint:', 'art', 'of', 'the', 'dog', 'collar”', 'this', 'illustration', 'dated', 'from', 'circa', '1880', '1895', 'shows', 'a', 'woman', 'standing', 'in', 'front', 'of', 'the', 'window', 'of', 'enoch', 'frères', '&', 'costallat', 'publishers', 'in', 'paris', 'with', 'in', 'the', 'foreground', 'a', 'poodle', 'this', 'illustration', 'dated', 'from', 'circa', '1880', '1895', 'shows', 'a', 'woman', 'standing', 'in', 'front', 'of', 'the', 'window', 'of', 'enoch', 'frères', '&', 'costallat', 'publishers', 'in', 'paris', 'with', 'in', 'the', 'foreground', 'a', 'poodle', 'sepia', 'times/universal', 'images', 'group/getty', 'images', 'the', 'traveling', 'exhibition', 'of', '187', 'collars', 'showed', 'the', '[MASK]', 'of', 'dogs', 'from', 'utilitarian', 'hunters', 'and', 'helpers', 'in', 'the', 'field', 'to', 'indoor', 'members', 'of', 'the', 'family', '“you', 'see', 'these', 'paintings', 'from', 'the', '17th', 'century', 'often', 'difficult', 'in', 'the', 'museum', 'to', 'display', 'that', 'are', 'of', 'bloody', 'violent', 'dogs”', 'fausel', 'said', '“then', 'you', 'turn', 'a', 'couple', 'of', 'pages', '[in', 'an', 'art', 'book]', 'and', 'all', 'of', 'a', 'sudden', 'there', 'are', 'these', 'same', 'dogs', 'in', 'your', 'living', 'room', 'so', 'it’s', 'domesticity', 'that', 'changes', 'the', 'arc', 'of', 'the', 'manufacture', 'and', 'the', 'quality', 'of', 'the', 'item”', 'in', 'the', '1600s', 'when', 'dogs', 'were', 'used', 'in', 'pack', 'hunts', 'of', 'large', 'quarry', '(bears', 'boars', 'stags', 'etc)', 'the', 'canines', 'would', 'wear', 'spiked', 'iron', 'collars', 'for', 'protection', 'in', 'the', 'coming', 'centuries', 'collars', 'took', 'on', 'a', 'softer', 'decorative', 'form', 'and', 'were', 'made', 'from', 'materials', 'such', 'as', 'brass', 'tiffany', 'silver', 'and', 'gemstones', 'in', 'recent', 'decades', 'there', 'have', 'been', 'some', 'memorable', 'canine', 'attire', 'examples', 'in', 'popular', 'culture:', 'bruiser', 'woods’', 'fetching', 'pinkhued', 'wardrobe', 'in', '“legally', 'blonde;”', 'frank', 'the', 'pug’s', 'sharp', 'suit', '&', 'tie', 'in', 'the', '“men', 'in', 'black”', 'films', 'or', 'eddie’s', 'bright', 'yellow', 'rain', 'slicker', 'on', 'a', 'stormy', 'seattle', 'episode', 'of', '“frasier”', 'and', 'scrolls', 'through', 'tiktok', 'and', 'instagram', 'today', 'show', 'fashionistas', 'and', 'paw', 'parents', 'alike', 'playing', 'dressup', 'with', 'their', 'pet', 'squirrels', 'miniature', 'pigs', 'and', 'the', 'most', 'tolerant', 'of', 'cats', 'the', 'dachshund', 'wears', 'prada', 'the', 'dog', 'apparel', 'business', 'of', '2024', 'offers', 'many', 'choices', 'and', 'caters', 'to', 'all', 'kinds', 'of', 'pet', 'owners:', 'one', 'could', 'pony', 'up', 'for', 'a', '$1000', 'gucci', 'or', 'prada', 'coat', 'or', 'spend', 'a', 'couple', 'of', 'ten', 'spots', 'for', 'a', 'weatherproof', 'coat', 'on', 'chewy', \"prada's\", 'renylon', 'puffer', 'dog', 'coat', 'with', 'hood', 'comes', 'in', 'two', 'colors', 'and', 'three', 'sizes', '$825', \"prada's\", 'renylon', 'puffer', 'dog', 'coat', 'with', 'hood', 'comes', 'in', 'two', 'colors', 'and', 'three', 'sizes', '$825', 'from', 'prada', 'but', 'the', 'transition', 'from', 'fifth', 'avenue', 'highend', 'and', 'high', 'fashion', 'to', 'highperformance', 'and', 'technical', 'function', 'has', 'certainly', 'accelerated', 'in', 'recent', 'decades', 'bend', 'oregonbased', 'ruffwear', 'is', 'one', 'member', 'of', 'a', 'large', 'pack', 'of', 'firms', 'that', 'sit', 'at', 'the', 'intersection', 'of', 'the', 'pet', 'products', 'industry', 'and', 'the', 'outdoor', 'gear', 'industry', '“we', 'approach', 'the', 'dog', 'apparel', 'as', 'we', 'would', 'outdoor', 'apparel', 'for', 'humans', 'in', 'the', 'sense', 'that', 'we’re', 'looking', 'for', 'outdoor', 'fabrications', 'outdoor', 'innovations', 'and', 'technologies', 'and', 'ways', 'to', 'be', 'able', 'to', 'contribute', 'to', 'the', 'comfort', 'of', 'any', 'of', 'our', 'dogs”', 'said', 'peter', 'kearns', 'ruffwear’s', 'director', 'of', 'product', 'batman', 'the', 'dog', 'and', 'his', 'human', 'in', 'the', 'backbone', 'trail', 'of', 'the', 'santa', 'monica', 'mountains', 'batman', 'wears', 'a', 'ruffwear', 'swamp', 'cooler', 'zip', 'vest', 'and', 'apparel', 'batman', 'the', 'dog', 'and', 'his', 'human', 'in', 'the', 'backbone', 'trail', 'of', 'the', 'santa', 'monica', 'mountains', 'batman', 'wears', 'a', 'ruffwear', 'swamp', 'cooler', 'zip', 'vest', 'and', 'apparel', 'courtesy', 'ruffwear', '“there’s', 'this', 'continued', 'trend', 'around', 'dogs', 'as', 'family”', 'he', 'added', '“and', 'when', 'taking', 'them', 'on', 'vacation', '[or', 'other', 'excursions]', 'you', 'need', 'to', 'be', 'prepared', 'and', 'you', 'need', 'to', 'prepare', 'your', 'dog”', 'pets', 'and', 'the', 'pandemic', 'since', '2020', 'that', 'trend', 'has', 'intensified', 'the', 'covid19', 'pandemic', 'resulted', 'in', 'people', 'spending', 'more', 'time', 'at', 'home', 'with', 'their', 'pets', 'as', 'well', 'as', 'adopting', 'new', 'companions;', 'as', 'a', 'result', 'pet', 'product', 'sales', 'boomed', 'prepandemic', 'spending', 'on', 'pets', 'had', 'increased', 'slowly', 'and', 'steadily', 'following', 'the', 'great', 'recession', 'growing', 'from', '$46', 'billion', 'in', '2009', 'to', '$75', 'billion', 'in', '2019', 'according', 'to', 'the', 'american', 'pet', 'products', 'association', 'the', 'pandemic', 'effect', 'was', 'huge:', 'spending', 'soared', 'to', '$104', 'billion', 'in', '2020', 'it’s', 'since', 'bounded', 'up', 'to', '$148', 'billion', 'to', 'end', '2023', 'and', 'is', 'expected', 'to', 'hit', 'nearly', '$280', 'billion', 'in', '2030', 'according', 'to', 'the', 'appa', 'which', 'does', 'not', 'break', 'out', 'apparel', 'sales', 'voyagers', 'k9', 'apparel', 'a', 'tacoma', 'washingtonbased', 'maker', 'of', 'jackets', 'for', '25', 'dog', 'breeds', 'has', 'seen', 'some', 'tailwinds', 'from', 'the', 'pandemic', 'three', 'dogs', 'in', 'voyagers', 'k9', 'apparel', 'yellow', 'raincoats', 'on', 'a', 'rocky', 'beach', 'on', 'a', 'grey', 'day', 'three', 'dogs', 'in', 'voyagers', 'k9', 'apparel', 'yellow', 'raincoats', 'on', 'a', 'rocky', 'beach', 'on', 'a', 'grey', 'day', 'courtesy', 'robyn', 'baldrey/@reddogsquad', '“people', 'were', 'adopting', 'more', 'dogs', 'and', 'people', 'were', 'home', 'with', 'their', 'dogs', 'more', 'but', 'the', 'other', 'one', 'that', 'i', 'think', 'we', 'sometimes', 'overlook', 'and', 'is', 'really', 'more', 'core', 'to', 'our', 'business', 'is', 'how', 'much', 'people', 'were', 'getting', 'outside', 'with', 'their', 'dogs”', 'said', 'joe', 'hafner', 'who', 'bought', 'the', 'company', 'last', 'year', 'from', 'its', 'longtime', 'owners', '“postpandemic', 'people', 'continue', 'to', 'hold', 'onto', 'that', 'love', 'and', 'respect', 'for', 'nature', 'and', 'taking', 'their', 'dogs', 'with', 'them', 'on', 'those', 'long', 'walks', 'those', 'long', 'hikes”', 'he', 'added', 'in', 'addition', 'to', 'the', 'simmering', 'consumer', 'trends', 'the', 'shifting', 'climate', 'patterns', '(including', 'truncated', 'winters)', 'and', 'more', 'extreme', 'weather', 'events', 'have', 'prompted', 'voyagers', 'to', 'introduce', 'a', 'line', 'of', 'cooling', 'coats', 'this', 'summer', 'he', 'said', '“if', 'your', 'intent', 'with', 'your', 'dog', 'is', 'to', 'protect', 'them', 'from', 'weather', 'and', 'protect', 'them', 'from', 'extreme', 'heat', 'or', 'extreme', 'cold', 'there’s', 'a', 'place', 'in', 'the', 'market', 'for', 'functional', 'wear', 'of', 'that', 'type”', 'hafner', 'said', '“that', 'being', 'said', 'there’s', 'plenty', 'of', 'great', 'reasons', 'to', 'dress', 'your', 'dog', 'up', 'for', 'halloween', 'or', 'a', 'walk', 'around', 'new', 'york', 'city', 'in', 'their', 'stately', 'duds', 'that’s', 'not', 'the', 'market', 'that', 'we', 'are', 'serving', 'but', 'i', 'think', 'there’s', 'certainly', 'a', 'place', 'for', 'that', 'too”', 'coats', 'for', 'peace', 'of', 'mind', 'at', 'nature', 'of', 'the', 'dog', 'boarding', 'and', 'daycare', 'facility', 'in', 'ada', 'michigan', 'owner', 'jackie', 'moord', 'has', 'seen', 'a', 'steady', 'increase', 'of', 'the', 'coatwearing', 'canine', 'clientele', '“there', 'are', 'a', 'lot', 'of', 'different', 'styles', 'for', 'these', 'jackets', 'now', 'so', 'maybe', 'it’s', 'some', 'of', 'that', 'which', 'is', 'kind', 'of', 'fun', '(for', 'owners)”', 'she', 'said', '“but', 'mainly', 'i', 'think', 'it', 'creates', 'peace', 'of', 'mind', '[MASK]', 'people', 'sending', 'your', 'dog', 'to', 'daycare', 'in', 'a', 'coat', 'just', 'kind', 'of', 'helps', 'with', 'that', 'peace', 'of', 'mind', 'that', 'they’re', 'taking', 'good', 'care', 'of', 'the', 'dog”', 'three', 'chihuahuas', 'wearing', 'matching', 'coats', 'watch', 'their', 'owner', 'from', 'a', 'stroller', 'on', 'november', '27', '2020', 'in', 'rehoboth', 'beach', 'delaware', 'three', 'chihuahuas', 'wearing', 'matching', 'coats', 'watch', 'their', 'owner', 'from', 'a', 'stroller', 'on', 'november', '27', '2020', 'in', 'rehoboth', 'beach', 'delaware', 'mark', 'makela/getty', 'images', 'in', 'anoka', 'minnesota', 'nicole', 'wheatley', 'has', 'made', 'custom', 'fleece', 'pullovers', 'for', 'dogs', 'since', '2012', 'that’s', 'when', 'she', 'crafted', 'her', 'first', 'coat', 'purely', 'out', 'of', 'necessity', 'when', 'her', 'doberman', 'shanoa', 'managed', 'to', 'chew', 'up', 'an', 'expensive', 'winter', 'coat', '“and', 'i', 'thought', '‘i', 'can’t', 'spend', 'that', 'kind', 'of', 'money', 'again’”', 'wheatley', 'said', 'the', 'selftaught', 'seamstress', 'sewed', 'together', 'some', 'fleece', 'to', 'keep', 'shanoa', 'warm', 'and', 'shared', 'the', 'final', 'results', 'on', 'the', 'doberman', 'owners’', 'forum', 'she', 'comoderated', '“they', 'were', 'like', '‘oh', 'those', 'look', 'nice', 'could', 'you', 'make', 'one', 'for', 'me’”', 'she', 'said', '“and', 'then', 'it', 'just', 'kind', 'of', 'spiraled”', 'homespun', 'threads', 'flashforward', 'to', '2024', 'and', 'the', 'onewoman', 'shop', 'of', 'made', 'by', 'meadowcat', 'is', 'now', 'wheatley’s', 'fulltime', 'job', 'she', 'produces', 'about', '1000', 'to', '1300', 'custom', 'coats', 'annually', 'priced', 'generally', 'between', '$40', 'and', '$75', 'that', 'are', 'made', 'for', 'a', 'variety', 'of', 'dogs', 'including', 'some', 'unlikely', 'breeds', 'for', 'example', 'wheatley', 'crafted', 'a', 'bright', 'fluorescent', 'orange', 'coat', 'for', 'a', 'resident', 'in', 'minnesota’s', 'northwoods', 'who', 'feared', 'their', 'husky', 'could', 'get', 'mistaken', 'for', 'a', 'wolf', 'over', 'in', 'muncy', 'pennsylvania', 'a', 'doberman', 'named', 'drogo', 'has', 'more', 'than', '50', 'made', '[MASK]', 'meadowcat', 'coats', 'said', 'lois', 'katchur', 'who', 'was', 'one', 'of', 'wheatley’s', 'first', 'customers', 'two', 'dobermans', 'in', 'made', 'by', \"meadowcat's\", 'blue', 'and', 'red', 'coordinating', 'coats', 'sypha', 'on', 'the', 'left', 'and', 'richter', 'on', 'the', 'right', 'two', 'dobermans', 'in', 'made', 'by', \"meadowcat's\", 'blue', 'and', 'red', 'coordinating', 'coats', 'sypha', 'on', 'the', 'left', 'and', 'richter', 'on', 'the', 'right', 'courtesy', 'nicole', 'wheatley', 'some', 'of', 'drogo’s', 'coats', 'are', 'handmedowns', 'from', 'katchur’s', 'previous', 'dobermans', 'and', 'many', 'are', 'tailored', 'for', 'the', '88pound', 'pup', 'named', 'after', 'the', '“game', 'of', 'thrones”', 'dothraki', 'warlord', '(one', 'coat', 'fittingly', 'features', 'firebreathing', 'dragons)', 'doberman', 'pinschers', 'along', 'with', 'sleek', 'breeds', 'like', 'greyhounds', 'and', 'whippets', 'are', 'prime', 'candidates', 'for', 'dog', 'coats', 'because', 'of', 'their', 'short', 'hair', 'thin', 'skin', 'and', 'minimal', 'body', 'fat', 'on', 'a', 'recent', 'february', 'morning', 'in', 'wheatley’s', 'home', 'office', 'her', 'dobermans', 'leon', 'and', 'sypha', 'contently', 'and', 'gently', 'chewed', 'on', 'handmade', 'fleece', 'patchwork', 'blanket', 'toys', 'sypha', 'did', 'so', 'wearing', 'a', 'grumpy', 'cat', 'pullover', '“she', 'was', 'chilly', 'this', 'morning”', 'wheatley', 'explained', '“she', 'was', 'sitting', 'next', 'to', 'me', 'shivering', 'and', 'looking', 'at', 'me”', 'tips', 'for', 'pet', 'parents', 'dr', 'jennifer', 'bruns', 'doctor', 'of', 'veterinary', 'medicine', 'at', 'petsmart', 'veterinary', 'services', 'said', 'it’s', 'important', 'to', 'monitor', 'your', '[MASK]', 'for', 'signs', 'such', 'as', 'shivering', 'or', 'general', 'discomfort', 'to', 'determine', 'when', 'a', 'coat', 'is', 'in', 'need', 'a', 'woman', 'runs', 'with', 'her', 'dog', 'along', 'the', 'snowcovered', 'national', 'mall', 'in', 'january', '2022', 'in', 'washington', 'dc', 'a', 'woman', 'runs', 'with', 'her', 'dog', 'along', 'the', 'snowcovered', 'national', 'mall', 'in', 'january', '2022', 'in', 'washington', 'dc', 'kent', 'nishimura/los', 'angeles', 'times/getty', 'images', '“in', 'all', 'situations', 'introduce', 'the', 'coat', 'slowly', 'as', 'it', 'might', 'take', 'a', 'few', 'times', 'for', 'your', 'dog', 'to', 'get', 'adjusted', 'to', 'wearing', 'it”', 'bruns', 'wrote', 'via', 'email', '“make', 'sure', 'the', 'coat', 'fits', 'well', 'for', 'safety', 'and', 'they’re', 'supervised', 'while', 'wearing', 'the', 'coat', 'consider', 'coats', 'with', 'reflectors', 'for', 'nighttime', 'safety”', 'coats', 'also', 'can', 'help', 'calm', 'a', 'dog', 'during', 'anxietyinducing', 'situations', 'like', 'the', 'fourth', 'of', 'july', 'or', 'keep', 'dry', 'during', 'rainstorms', 'beyond', 'coats', 'winter', 'wear', 'such', 'as', 'boots', 'are', 'beneficial', 'in', 'protecting', 'paws', 'from', 'frostbite', 'or', 'injuries', 'from', 'chemical', 'deicers', 'klein', 'added', 'akc', 'has', 'additional', 'tips', 'on', 'its', 'website', 'but', 'also', 'recommends', 'to', 'never', 'leave', 'your', 'dog', 'unattended', 'in', 'a', 'car', 'no', 'matter', 'the', 'season', '[SEP]']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "masked tokens (words) :  ['of', 'by', 'progression', 'for', 'dog']\n",
            "masked tokens list :  [69, 317, 664, 228, 657]\n",
            "masked tokens (words) :  ['begs', 'begs', 'begs', 'begs', 'begs']\n",
            "predict masked tokens list :  [38, 38, 38, 38, 38]\n",
            "0\n",
            "isNext :  False\n",
            "predict isNext :  False\n"
          ]
        }
      ],
      "source": [
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[2]))\n",
        "print([id2word[w.item()] for w in input_ids[0] if id2word[w.item()] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
        "#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\n",
        "#logits_nsp: (1, yes/no) ==> (1, 2)\n",
        "\n",
        "#predict masked tokens\n",
        "#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy() \n",
        "#note that zero is padding we add to the masked_tokens\n",
        "print('masked tokens (words) : ',[id2word[pos.item()] for pos in masked_tokens[0]])\n",
        "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0]])\n",
        "print('masked tokens (words) : ',[id2word[pos.item()] for pos in logits_lm])\n",
        "print('predict masked tokens list : ', [pos for pos in logits_lm])\n",
        "\n",
        "#predict nsp\n",
        "logits_nsp = logits_nsp.data.max(1)[1][0].data.numpy()\n",
        "print(logits_nsp)\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ',True if logits_nsp else False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "# Set GPU device\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
        "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train, Test, Validation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the latest cached version of the dataset since snli couldn't be found on the Hugging Face Hub\n",
            "Found the latest cached dataset configuration 'plain_text' at C:\\Users\\thama\\.cache\\huggingface\\datasets\\snli\\plain_text\\1.0.0\\660623b4423e96e71317e24b66ec156855dcb5d4 (last modified on Sun Mar  3 11:13:22 2024).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'premise': Value(dtype='string', id=None),\n",
              " 'hypothesis': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import datasets\n",
        "snli = datasets.load_dataset('snli')\n",
        "# mnli = datasets.load_dataset('glue', 'mnli')\n",
        "# mnli['train'].features \n",
        "snli['train'].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# there are -1 values in the label feature, these are where no class could be decided so we remove\n",
        "snli = snli.filter(\n",
        "    lambda x: 0 if x['label'] == -1 else 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.unique(snli['train']['label'])\n",
        "#snli also have -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label'],\n",
              "        num_rows: 10\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create Dataset from snli\n",
        "from datasets import DatasetDict\n",
        "raw_dataset = DatasetDict({\n",
        "    'train': snli['train'].shuffle(seed=55).select(list(range(100))),\n",
        "    'test': snli['test'].shuffle(seed=55).select(list(range(10))),\n",
        "    'validation': snli['validation'].shuffle(seed=55).select(list(range(100)))\n",
        "})\n",
        "raw_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "#for task3\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenizer_config.json: 100%|██████████| 49.0/49.0 [00:00<?, ?B/s]\n",
            "c:\\Users\\thama\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\thama\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 401kB/s]\n",
            "tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 475kB/s]\n"
          ]
        }
      ],
      "source": [
        "#for task3\n",
        "tokenizer2 = BertTokenizer.from_pretrained('bert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 100/100 [00:00<00:00, 3002.56 examples/s]\n",
            "Map: 100%|██████████| 10/10 [00:00<00:00, 407.61 examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 4228.04 examples/s]\n"
          ]
        }
      ],
      "source": [
        "#preprocess function for bert-base-uncased model\n",
        "def preprocess_function(examples):\n",
        "    max_seq_length = 128\n",
        "    padding = 'max_length'\n",
        "    #randomly choose two sentence\n",
        "    tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
        "    tokens_a, tokens_b            = token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "        \n",
        "        #1. token embedding - add CLS and SEP\n",
        "    input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
        "        \n",
        "        #2. segment embedding - which sentence is 0 and 1\n",
        "    segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "        \n",
        "        #3 masking\n",
        "    n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
        "        #get all the pos excluding CLS and SEP\n",
        "    candidates_masked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] \n",
        "                                 and token != word2id['[SEP]']]\n",
        "    shuffle(candidates_masked_pos)\n",
        "    masked_tokens, masked_pos = [], []\n",
        "        #simply loop and mask accordingly\n",
        "    for pos in candidates_masked_pos[:n_pred]:\n",
        "        masked_pos.append(pos)\n",
        "        masked_tokens.append(input_ids[pos])\n",
        "        if random() < 0.1:  #10% replace with random token\n",
        "            index = randint(0, vocab_size - 1)\n",
        "            input_ids[pos] = word2id[id2word[index]]\n",
        "        elif random() < 0.8:  #80 replace with [MASK]\n",
        "            input_ids[pos] = word2id['[MASK]']\n",
        "        else: \n",
        "            pass\n",
        "            \n",
        "        #4. pad the sentence to the max length\n",
        "    n_pad = max_len - len(input_ids)\n",
        "    input_ids.extend([0] * n_pad)\n",
        "    segment_ids.extend([0] * n_pad)\n",
        "        \n",
        "        #5. pad the mask tokens to the max length\n",
        "    if max_mask > n_pred:\n",
        "        n_pad = max_mask - n_pred\n",
        "        masked_tokens.extend([0] * n_pad)\n",
        "        masked_pos.extend([0] * n_pad)\n",
        "    return {\n",
        "        # \"premise_input_ids\": premise_result[\"input_ids\"],\n",
        "        # \"premise_attention_mask\": premise_result[\"attention_mask\"],\n",
        "        # \"hypothesis_input_ids\": hypothesis_result[\"input_ids\"],\n",
        "        # \"hypothesis_attention_mask\": hypothesis_result[\"attention_mask\"],\n",
        "        # \"labels\" : labels\n",
        "    }\n",
        "\n",
        "tokenized_datasets = raw_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        ")\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['premise','hypothesis','label'])\n",
        "tokenized_datasets.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#preprocess function for bert-base-cased model\n",
        "def preprocess_function2(examples):\n",
        "    max_seq_length = 128\n",
        "    padding = 'max_length'\n",
        "    # Tokenize the premise\n",
        "    premise_result = tokenizer2(\n",
        "        examples['premise'], padding=padding, max_length=max_seq_length, truncation=True)\n",
        "    #num_rows, max_seq_length\n",
        "    # Tokenize the hypothesis\n",
        "    hypothesis_result = tokenizer2(\n",
        "        examples['hypothesis'], padding=padding, max_length=max_seq_length, truncation=True)\n",
        "    #num_rows, max_seq_length\n",
        "    # Extract labels\n",
        "    labels = examples[\"label\"]\n",
        "    #num_rows\n",
        "    return {\n",
        "        \"premise_input_ids\": premise_result[\"input_ids\"],\n",
        "        \"premise_attention_mask\": premise_result[\"attention_mask\"],\n",
        "        \"hypothesis_input_ids\": hypothesis_result[\"input_ids\"],\n",
        "        \"hypothesis_attention_mask\": hypothesis_result[\"attention_mask\"],\n",
        "        \"labels\" : labels\n",
        "    }\n",
        "\n",
        "tokenized_datasets2 = raw_dataset.map(\n",
        "    preprocess_function2,\n",
        "    batched=True,\n",
        ")\n",
        "\n",
        "tokenized_datasets2 = tokenized_datasets2.remove_columns(['premise','hypothesis','label'])\n",
        "tokenized_datasets2.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
              "        num_rows: 10\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#tokenized dataset of bert-base-uncased model\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
              "        num_rows: 10\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#tokenized dataset of bert-base-cased model\n",
        "tokenized_datasets2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "#dataloader of bert-base-uncased model\n",
        "# initialize the dataloader\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets['train'], \n",
        "    batch_size=batch_size, \n",
        "    shuffle=True\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets['validation'], \n",
        "    batch_size=batch_size\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    tokenized_datasets['test'], \n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "#dataloader of bert-base-cased model\n",
        "# initialize the dataloader\n",
        "batch_size = 32\n",
        "train_dataloader2 = DataLoader(\n",
        "    tokenized_datasets2['train'], \n",
        "    batch_size=batch_size, \n",
        "    shuffle=True\n",
        ")\n",
        "eval_dataloader2 = DataLoader(\n",
        "    tokenized_datasets2['validation'], \n",
        "    batch_size=batch_size\n",
        ")\n",
        "test_dataloader2 = DataLoader(\n",
        "    tokenized_datasets2['test'], \n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "for batch in train_dataloader:\n",
        "    print(batch['premise_input_ids'].shape)\n",
        "    print(batch['premise_attention_mask'].shape)\n",
        "    print(batch['hypothesis_input_ids'].shape)\n",
        "    print(batch['hypothesis_attention_mask'].shape)\n",
        "    print(batch['labels'].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "for batch in train_dataloader2:\n",
        "    print(batch['premise_input_ids'].shape)\n",
        "    print(batch['premise_attention_mask'].shape)\n",
        "    print(batch['hypothesis_input_ids'].shape)\n",
        "    print(batch['hypothesis_attention_mask'].shape)\n",
        "    print(batch['labels'].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BERT(\n",
              "  (embedding): Embedding(\n",
              "    (tok_embed): Embedding(852, 768)\n",
              "    (pos_embed): Embedding(3909, 768)\n",
              "    (seg_embed): Embedding(2, 768)\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (layers): ModuleList(\n",
              "    (0-5): 6 x EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (W_Q): Linear(in_features=768, out_features=512, bias=True)\n",
              "        (W_K): Linear(in_features=768, out_features=512, bias=True)\n",
              "        (W_V): Linear(in_features=768, out_features=512, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PoswiseFeedForwardNet(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (activ): Tanh()\n",
              "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (decoder): Linear(in_features=768, out_features=852, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#use BERT model with snli data\n",
        "model2 = BERT()\n",
        "model2.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pooling\n",
        "SBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define mean pooling function\n",
        "def mean_pool(token_embeds, attention_mask):\n",
        "    # reshape attention_mask to cover 768-dimension embeddings\n",
        "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
        "        token_embeds.size()\n",
        "    ).float()\n",
        "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
        "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
        "        in_mask.sum(1), min=1e-9\n",
        "    )\n",
        "    return pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Loss Function\n",
        "\n",
        "## Classification Objective Function \n",
        "We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference  $\\lvert u - v \\rvert $ and multiply the result with the trainable weight  $ W_t ∈  \\mathbb{R}^{3n \\times k}  $:\n",
        "\n",
        "$ o = \\text{softmax}\\left(W^T \\cdot \\left(u, v, \\lvert u - v \\rvert\\right)\\right) $\n",
        "\n",
        "where $n$ is the dimension of the sentence embeddings and k the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n",
        "\n",
        "## Regression Objective Function. \n",
        "The cosine similarity between the two sentence embeddings $u$ and $v$ is computed (Figure 2). We use means quared-error loss as the objective function.\n",
        "\n",
        "(Manhatten / Euclidean distance, semantically  similar sentences can be found.)\n",
        "\n",
        "<img src=\"./figures/sbert-architecture.png\" >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def configurations(u,v):\n",
        "    # build the |u-v| tensor\n",
        "    uv = torch.sub(u, v)   # batch_size,hidden_dim\n",
        "    uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
        "    \n",
        "    # concatenate u, v, |u-v|\n",
        "    x = torch.cat([u, v, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
        "    return x\n",
        "\n",
        "def cosine_similarity(u, v):\n",
        "    dot_product = np.dot(u, v)\n",
        "    norm_u = np.linalg.norm(u)\n",
        "    norm_v = np.linalg.norm(v)\n",
        "    similarity = dot_product / (norm_u * norm_v)\n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./figures/sbert-ablation.png\" width=\"350\" height=\"300\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_head = torch.nn.Linear(768*3, 3).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=2e-5)\n",
        "optimizer_classifier = torch.optim.Adam(classifier_head.parameters(), lr=2e-5)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\thama\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        }
      ],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# and setup a warmup for the first ~10% steps\n",
        "total_steps = int(len(raw_dataset) / batch_size)\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "\t\toptimizer, num_warmup_steps=warmup_steps,\n",
        "  \tnum_training_steps=total_steps - warmup_steps\n",
        ")\n",
        "\n",
        "# then during the training loop we update the scheduler per step\n",
        "scheduler.step()\n",
        "\n",
        "scheduler_classifier = get_linear_schedule_with_warmup(\n",
        "\t\toptimizer_classifier, num_warmup_steps=warmup_steps,\n",
        "  \tnum_training_steps=total_steps - warmup_steps\n",
        ")\n",
        "\n",
        "# then during the training loop we update the scheduler per step\n",
        "scheduler_classifier.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "num_epoch = 1\n",
        "# 1 epoch should be enough, increase if wanted\n",
        "for epoch in range(num_epoch):\n",
        "    model2.train()  \n",
        "    classifier_head.train()\n",
        "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, leave=True)):\n",
        "        # zero all gradients on each new step\n",
        "        optimizer.zero_grad()\n",
        "        optimizer_classifier.zero_grad()\n",
        "        \n",
        "        # prepare batches and more all to the active device\n",
        "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
        "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
        "        attention_a = batch['premise_attention_mask'].to(device)\n",
        "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
        "        label = batch['labels'].to(device)\n",
        "        \n",
        "        # extract token embeddings from BERT at last_hidden_state\n",
        "        u,_ = model2(inputs_ids_a, attention_mask=attention_a)  \n",
        "        v,_ = model2(inputs_ids_b, attention_mask=attention_b)  \n",
        "\n",
        "        u_last_hidden_state = u.last_hidden_state # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "        v_last_hidden_state = v.last_hidden_state # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "         # get the mean pooled vectors\n",
        "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
        "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
        "        \n",
        "        # build the |u-v| tensor\n",
        "        uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
        "        uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
        "        \n",
        "        # concatenate u, v, |u-v|\n",
        "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
        "        \n",
        "        # process concatenated tensor through classifier_head\n",
        "        x = classifier_head(x) #batch_size, classifer\n",
        "        \n",
        "        # calculate the 'softmax-loss' between predicted and true label\n",
        "        loss = criterion(x, label)\n",
        "        \n",
        "        # using loss, calculate gradients and then optimizerize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer_classifier.step()\n",
        "\n",
        "        scheduler.step() # update learning rate scheduler\n",
        "        scheduler_classifier.step()\n",
        "        \n",
        "    # print(f'Epoch: {epoch + 1} | loss = {loss.item():.6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Cosine Similarity: 0.7661\n"
          ]
        }
      ],
      "source": [
        "model2.eval()\n",
        "classifier_head.eval()\n",
        "total_similarity = 0\n",
        "with torch.no_grad():\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        # prepare batches and more all to the active device\n",
        "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
        "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
        "        attention_a = batch['premise_attention_mask'].to(device)\n",
        "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
        "        label = batch['labels'].to(device)\n",
        "        \n",
        "        # extract token embeddings from BERT at last_hidden_state\n",
        "        u = model2(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "        v = model2(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "        # get the mean pooled vectors\n",
        "        u_mean_pool = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1) # batch_size, hidden_dim\n",
        "        v_mean_pool = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1) # batch_size, hidden_dim\n",
        "\n",
        "        similarity_score = cosine_similarity(u_mean_pool, v_mean_pool)\n",
        "        total_similarity += similarity_score\n",
        "    \n",
        "average_similarity = total_similarity / len(eval_dataloader)\n",
        "print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 0.8057\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity(model, tokenizer, sentence_a, sentence_b, device):\n",
        "    # Tokenize and convert sentences to input IDs and attention masks\n",
        "    inputs_a = tokenizer(sentence_a, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "    inputs_b = tokenizer(sentence_b, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "\n",
        "    # Move input IDs and attention masks to the active device\n",
        "    inputs_ids_a = inputs_a['input_ids']\n",
        "    attention_a = inputs_a['attention_mask']\n",
        "    inputs_ids_b = inputs_b['input_ids']\n",
        "    attention_b = inputs_b['attention_mask']\n",
        "\n",
        "    # Extract token embeddings from BERT\n",
        "    u = model(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "    v = model(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "    # Get the mean-pooled vectors\n",
        "    u = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
        "    v = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_score = cosine_similarity(u.reshape(1, -1), v.reshape(1, -1))[0, 0]\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Example usage:\n",
        "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
        "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
        "similarity = calculate_similarity(model2, tokenizer, sentence_a, sentence_b, device)\n",
        "print(f\"Cosine Similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_head = torch.nn.Linear(768*3, 3).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model3.parameters(), lr=2e-5)\n",
        "optimizer_classifier = torch.optim.Adam(classifier_head.parameters(), lr=2e-5)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# start from a pretrained bert-base-uncased model\n",
        "from transformers import BertTokenizer, BertModel\n",
        "model3 = BertModel.from_pretrained('bert-base-uncased')\n",
        "model3.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [01:31<00:00, 22.81s/it]\n",
            "100%|██████████| 4/4 [01:26<00:00, 21.53s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "num_epoch = 2\n",
        "# 1 epoch should be enough, increase if wanted\n",
        "for epoch in range(num_epoch):\n",
        "    model3.train()  \n",
        "    classifier_head.train()\n",
        "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, leave=True)):\n",
        "        # zero all gradients on each new step\n",
        "        optimizer.zero_grad()\n",
        "        optimizer_classifier.zero_grad()\n",
        "        \n",
        "        # prepare batches and more all to the active device\n",
        "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
        "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
        "        attention_a = batch['premise_attention_mask'].to(device)\n",
        "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
        "        label = batch['labels'].to(device)\n",
        "        \n",
        "        # extract token embeddings from BERT at last_hidden_state\n",
        "        u = model3(inputs_ids_a, attention_mask=attention_a)  \n",
        "        v = model3(inputs_ids_b, attention_mask=attention_b)  \n",
        "\n",
        "        u_last_hidden_state = u.last_hidden_state # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "        v_last_hidden_state = v.last_hidden_state # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "         # get the mean pooled vectors\n",
        "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
        "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
        "        \n",
        "        # build the |u-v| tensor\n",
        "        uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
        "        uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
        "        \n",
        "        # concatenate u, v, |u-v|\n",
        "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
        "        \n",
        "        # process concatenated tensor through classifier_head\n",
        "        x = classifier_head(x) #batch_size, classifer\n",
        "        \n",
        "        # calculate the 'softmax-loss' between predicted and true label\n",
        "        loss = criterion(x, label)\n",
        "        \n",
        "        # using loss, calculate gradients and then optimizerize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer_classifier.step()\n",
        "\n",
        "        scheduler.step() # update learning rate scheduler\n",
        "        scheduler_classifier.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Cosine Similarity: 0.7784\n"
          ]
        }
      ],
      "source": [
        "model3.eval()\n",
        "classifier_head.eval()\n",
        "total_similarity = 0\n",
        "with torch.no_grad():\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        # prepare batches and more all to the active device\n",
        "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
        "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
        "        attention_a = batch['premise_attention_mask'].to(device)\n",
        "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
        "        label = batch['labels'].to(device)\n",
        "        \n",
        "        # extract token embeddings from BERT at last_hidden_state\n",
        "        u = model3(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "        v = model3(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "        # get the mean pooled vectors\n",
        "        u_mean_pool = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1) # batch_size, hidden_dim\n",
        "        v_mean_pool = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1) # batch_size, hidden_dim\n",
        "\n",
        "        similarity_score = cosine_similarity(u_mean_pool, v_mean_pool)\n",
        "        total_similarity += similarity_score\n",
        "    \n",
        "average_similarity = total_similarity / len(eval_dataloader)\n",
        "print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 0.8057\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity(model, tokenizer, sentence_a, sentence_b, device):\n",
        "    # Tokenize and convert sentences to input IDs and attention masks\n",
        "    inputs_a = tokenizer(sentence_a, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "    inputs_b = tokenizer(sentence_b, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "\n",
        "    # Move input IDs and attention masks to the active device\n",
        "    inputs_ids_a = inputs_a['input_ids']\n",
        "    attention_a = inputs_a['attention_mask']\n",
        "    inputs_ids_b = inputs_b['input_ids']\n",
        "    attention_b = inputs_b['attention_mask']\n",
        "\n",
        "    # Extract token embeddings from BERT\n",
        "    u = model(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "    v = model(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "    # Get the mean-pooled vectors\n",
        "    u = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
        "    v = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_score = cosine_similarity(u.reshape(1, -1), v.reshape(1, -1))[0, 0]\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Example usage:\n",
        "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
        "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
        "similarity = calculate_similarity(model3, tokenizer, sentence_a, sentence_b, device)\n",
        "print(f\"Cosine Similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# start from a pretrained bert-base-cased model\n",
        "from transformers import BertTokenizer, BertModel\n",
        "model4 = BertModel.from_pretrained('bert-base-cased')\n",
        "model4.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_head = torch.nn.Linear(768*3, 3).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model4.parameters(), lr=2e-5)\n",
        "optimizer_classifier = torch.optim.Adam(classifier_head.parameters(), lr=2e-5)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\thama\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        }
      ],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# and setup a warmup for the first ~10% steps\n",
        "total_steps = int(len(raw_dataset) / batch_size)\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "\t\toptimizer, num_warmup_steps=warmup_steps,\n",
        "  \tnum_training_steps=total_steps - warmup_steps\n",
        ")\n",
        "\n",
        "# then during the training loop we update the scheduler per step\n",
        "scheduler.step()\n",
        "\n",
        "scheduler_classifier = get_linear_schedule_with_warmup(\n",
        "\t\toptimizer_classifier, num_warmup_steps=warmup_steps,\n",
        "  \tnum_training_steps=total_steps - warmup_steps\n",
        ")\n",
        "\n",
        "# then during the training loop we update the scheduler per step\n",
        "scheduler_classifier.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [01:47<00:00, 26.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | loss = 1.100021\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [01:49<00:00, 27.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 | loss = 1.124104\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "num_epoch = 2\n",
        "# 1 epoch should be enough, increase if wanted\n",
        "for epoch in range(num_epoch):\n",
        "    model4.train()  \n",
        "    classifier_head.train()\n",
        "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
        "    for step, batch in enumerate(tqdm(train_dataloader2, leave=True)):\n",
        "        # zero all gradients on each new step\n",
        "        optimizer.zero_grad()\n",
        "        optimizer_classifier.zero_grad()\n",
        "        \n",
        "        # prepare batches and more all to the active device\n",
        "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
        "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
        "        attention_a = batch['premise_attention_mask'].to(device)\n",
        "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
        "        label = batch['labels'].to(device)\n",
        "        \n",
        "        # extract token embeddings from BERT at last_hidden_state\n",
        "        u = model4(inputs_ids_a, attention_mask=attention_a)  \n",
        "        v = model4(inputs_ids_b, attention_mask=attention_b)  \n",
        "\n",
        "        u_last_hidden_state = u.last_hidden_state # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "        v_last_hidden_state = v.last_hidden_state # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "         # get the mean pooled vectors\n",
        "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
        "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
        "        \n",
        "        # build the |u-v| tensor\n",
        "        uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
        "        uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
        "        \n",
        "        # concatenate u, v, |u-v|\n",
        "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
        "        \n",
        "        # process concatenated tensor through classifier_head\n",
        "        x = classifier_head(x) #batch_size, classifer\n",
        "        \n",
        "        # calculate the 'softmax-loss' between predicted and true label\n",
        "        loss = criterion(x, label)\n",
        "        \n",
        "        # using loss, calculate gradients and then optimizerize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer_classifier.step()\n",
        "\n",
        "        scheduler.step() # update learning rate scheduler\n",
        "        scheduler_classifier.step()\n",
        "        \n",
        "    print(f'Epoch: {epoch + 1} | loss = {loss.item():.6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Cosine Similarity: 0.8679\n"
          ]
        }
      ],
      "source": [
        "model4.eval()\n",
        "classifier_head.eval()\n",
        "total_similarity = 0\n",
        "with torch.no_grad():\n",
        "    for step, batch in enumerate(eval_dataloader2):\n",
        "        # prepare batches and more all to the active device\n",
        "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
        "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
        "        attention_a = batch['premise_attention_mask'].to(device)\n",
        "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
        "        label = batch['labels'].to(device)\n",
        "        \n",
        "        # extract token embeddings from BERT at last_hidden_state\n",
        "        u = model4(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "        v = model4(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "        # get the mean pooled vectors\n",
        "        u_mean_pool = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1) # batch_size, hidden_dim\n",
        "        v_mean_pool = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1) # batch_size, hidden_dim\n",
        "\n",
        "        similarity_score = cosine_similarity(u_mean_pool, v_mean_pool)\n",
        "        total_similarity += similarity_score\n",
        "    \n",
        "average_similarity = total_similarity / len(eval_dataloader2)\n",
        "print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 0.8880\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity(model, tokenizer, sentence_a, sentence_b, device):\n",
        "    # Tokenize and convert sentences to input IDs and attention masks\n",
        "    inputs_a = tokenizer(sentence_a, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "    inputs_b = tokenizer(sentence_b, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "\n",
        "    # Move input IDs and attention masks to the active device\n",
        "    inputs_ids_a = inputs_a['input_ids']\n",
        "    attention_a = inputs_a['attention_mask']\n",
        "    inputs_ids_b = inputs_b['input_ids']\n",
        "    attention_b = inputs_b['attention_mask']\n",
        "\n",
        "    # Extract token embeddings from BERT\n",
        "    u = model(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "    v = model(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "    # Get the mean-pooled vectors\n",
        "    u = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
        "    v = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_score = cosine_similarity(u.reshape(1, -1), v.reshape(1, -1))[0, 0]\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Example usage:\n",
        "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
        "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
        "similarity = calculate_similarity(model4, tokenizer2, sentence_a, sentence_b, device)\n",
        "print(f\"Cosine Similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To analyze hyperparameters choices,\n",
        "1. **Learning Rate**: Too high, and the model might overshoot the optimal parameters, causing instability or divergence. Too low, and the training might be slow. In this experiment, I use learning rate = 2e-5 following the papers.\n",
        "\n",
        "2. **Batch Size**: Larger batch sizes often lead to faster convergence because they provide more stable estimates of the gradient. However, large batch sizes require more memory and may lead to suboptimal generalization. Smaller batch sizes introduce more noise but can lead to better generalization. I use batch_size = 32 because too big batch size will require more memory, and I don't want the model too generalize.\n",
        "\n",
        "3. **Number of Epochs**: In the experiment, I use epoch = 2 since pretrained model spend a lot of time to train.\n",
        "\n",
        "4. **Optimizer Choice and Parameters**: Different optimizers have different update rules and hyperparameters that can significantly impact training dynamics. In the experiment, I use Adam optimizer because it adapts the learning rate for each parameter. \n",
        "\n",
        "5. **Dropout Rate**: Dropout is a regularization technique used to prevent overfitting by randomly dropping units (along with their connections) during training. The dropout rate determines the probability of dropping units. Higher dropout rates provide stronger regularization but may hinder learning if set too high.In the experiment, dropout = 0.1 according to pretrained model.\n",
        "\n",
        "6. **Warmup Steps**: Warmup steps are used in learning rate schedules to gradually increase the learning rate at the beginning of training. This can stabilize training and prevent divergence. The number of warmup steps influences how quickly the learning rate increases and its overall shape during training. In the experiment, I set warm up for the first 10% steps\n",
        "\n",
        "7. **Attention Masking**: BERT uses attention mechanisms to capture contextual information. The choice of attention mechanism and its parameters (e.g., number of attention heads, hidden layer size) can affect the model's ability to capture long-range dependencies and contextual information efficiently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During the implementation, several limitations and challenges may arise, along with potential improvements or modifications:\n",
        "\n",
        "1. **Computational Resources**: Training large BERT models requires substantial computational resources, including high-performance GPUs and significant memory. My environment have limitations about resource so that I cannot do the experiment as much as I want. To improve performance, I suggest to change environment, apply parallelism, find other model that requires less memory.\n",
        "2. **Data Preprocessing**: Preprocessing text data for BERT input involves tokenization, padding, and attention masking, which can be complex and time-consuming, especially for large datasets. I can use only small datasets otherwise, I will not able to train data.\n",
        "3. **Overfitting**: BERT models are prone to overfitting, especially when trained on small datasets or when using complex architectures. This problem can solve by use larger datasets.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
